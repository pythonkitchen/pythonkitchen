<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    a {
	word-wrap: break-word !important;
}
  </style>
  
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>

<link rel="stylesheet" href="/post.css"> 

</head>
<body class="bg-gray-50">

<nav class="bg-white border-gray-200 dark:bg-gray-900">
    <div class="max-w-screen-xl flex flex-wrap items-center justify-between mx-auto p-4">
      <a href="/" class="flex items-center">
          <img src="https://www.pythonkitchen.com/wp-content/uploads/2022/11/cropped-p3-50x50.png" class="h-8 mr-3" alt="PythonKitchen Logo" />
          <span class="self-center text-2xl font-semibold whitespace-nowrap dark:text-white">PythonKitchen</span>
      </a>
      <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="navbar-default" aria-expanded="false">
          <span class="sr-only">Open main menu</span>
          <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14">
              <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"/>
          </svg>
      </button>
      <div class="hidden w-full md:block md:w-auto" id="navbar-default">
        <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50 md:flex-row md:space-x-8 md:mt-0 md:border-0 md:bg-white dark:bg-gray-800 md:dark:bg-gray-900 dark:border-gray-700">
          <li>
            <a href="/" 
            class="block py-2 pl-3 pr-4 text-white bg-blue-700 rounded md:bg-transparent md:text-blue-700 md:p-0 dark:text-white md:dark:text-blue-500" aria-current="page">
            Home</a>
          </li>
          <li>
            <a href="https://github.com/pythonkitchen/pythonkitchen/tree/source/data/posts" class="block py-2 pl-3 pr-4 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0 dark:text-white md:dark:hover:text-blue-500 dark:hover:bg-gray-700 dark:hover:text-white md:dark:hover:bg-transparent">
              Write</a>
          </li>
          <li>
            <a href="https://forms.gle/gf1p1Aitw9M9m8Zy5" class="block py-2 pl-3 pr-4 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0 dark:text-white md:dark:hover:text-blue-500 dark:hover:bg-gray-700 dark:hover:text-white md:dark:hover:bg-transparent">
              Contact</a>
          </li>
          <!-- <li>
            <a href="#" class="block py-2 pl-3 pr-4 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0 dark:text-white md:dark:hover:text-blue-500 dark:hover:bg-gray-700 dark:hover:text-white md:dark:hover:bg-transparent">Pricing</a>
          </li>
          <li>
            <a href="#" class="block py-2 pl-3 pr-4 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0 dark:text-white md:dark:hover:text-blue-500 dark:hover:bg-gray-700 dark:hover:text-white md:dark:hover:bg-transparent">Contact</a>
          </li> -->
        </ul>
      </div>
    </div>
  </nav>
  
  

<div class="p-8">

</div>

<div class="flex flex-row space-x-2">
    <div class="basis-3/4">
        <div class="p-8 pl-12">
            <div class="ml-2">
                <div class=" p-12 bg-white rounded-lg shadow-lg">
                    <p class="text-3xl" style="font-weight: 600;">Principle Component Analysis in Machine Learning</p>
                    <br>
                    <div class="flex items-center gap-x-4 text-xs">
                        <time datetime="2023-02-18 03:49:19" class="text-gray-500">February 18, 2023</time>
                        
                        <a href="#" class="relative z-10 rounded-full bg-gray-50 px-3 py-1.5 font-medium text-gray-600 hover:bg-gray-100">
                            data science
                        </a>
                        
                        <a href="#" class="relative z-10 rounded-full bg-gray-50 px-3 py-1.5 font-medium text-gray-600 hover:bg-gray-100">
                            machine learning
                        </a>
                        
                    </div>
                    
                        <div class="relative mt-8 flex items-center gap-x-4">
                            <img src="https://github.com/ParthShukla211.png?size=200" alt="" class="h-10 w-10 rounded-full bg-gray-50">
                            <div class="text-sm leading-6">
                            <p class="font-semibold text-gray-900">
                                <a href="#">
                                <span class="absolute inset-0"></span>
                                Parth Shukla
                                </a>
                            </p>
                            <p class="text-gray-600">Ml wizard</p>
                            </div>
                        </div>
                    
                    <br>
                    <article>
                        <p>In machine learning to achieve higher accuracies and performance of the model, the <strong>data quality</strong> plays a major role while enhancing the model. If good quality data with meaningful features are provided to the machine learning model, then the model can be very accurate and reliable with lesser computational powers.</p>
<p>Principle Component Analysis, or PCA, is also one of the techniques that are used to enhance the quality of the data in terms of the dimensionality of the data. It is one of the famous <strong>dimensionality reduction</strong> techniques used in machine learning to reduce the dimensionality of the data and then the computational power needed to train the data.</p>
<p>In this article, we will discuss the principle component analysis, its working mechanism, its importance, and its application ns with good examples. This article will help one to understand the technique better and will be able to apply it if needed.</p>
<p>So, before directly diving into the principle component analysis, let us discuss the <strong>Curse of Dimensionality</strong>.</p>
<h1>Table of Contents</h1>
<ol>
<li>Curse of Dimensionality</li>
<li>What is Principle Component Analysis?</li>
<li>How Does PCA Works?</li>
<li>Key Takeaways</li>
<li>Conclusion</li>
</ol>
<h1>Curse of Dimensionality</h1>
<p>In machine learning, majorly we have two types of features, independent features, and dependent features. Independent features are those that are independent of each other and which are used for model training, while dependent features are those which are target variable and which is to be predicted by the model.</p>
<p>Now sometimes we have a dataset with lots of features that can be also called dimensions. In such cases the computational power of the model becomes very high which causes the slowness of the model and sometimes, it can affect the <strong>quality and performance</strong> of the model too.</p>
<p>For example, we know that KNN is a distance-based algorithm that works on the principle of calculating the distances between points or data points. Now here, the computations will be very low if the dimension of the data is only two, but the computations become complex and time-consuming when the dimension increases.</p>
<p><strong>Two dimensions:</strong> d = sqrt ((Y2 - Y1)^2 + (X2 - X1)^2)
<strong>Three Dimensions:</strong> d = sqrt ((Z2 - Z1)^2 + (Y2 - Y1)^2 + (X2 - X1)^2)</p>
<p>We noticed in the example that the computational power increases with the dimensions of the dataset which can be considered as the curse for the model and its performance, which phenomenon is known as the curse of dimensionality.</p>
<h1>What is the Principle Component Analysis?</h1>
<p>As we know that the higher dimensionality of the data is a curse for the model in some ways, then it is necessary to reduce the dimensionality of the data in order to achieve higher <strong>accuracies</strong> and a <strong>reliable model</strong>; Still, we can not directly drop some of the features from the dataset as all the features can contain some values of the dataset which can not be missed.</p>
<p>In such cases the principle component analysis comes to the rescue; it is a technique that is used for reducing the dimensionality of the data without losing the essence of the data and without losing any of the information from the dataset.</p>
<p>Principle component analysis or the PCA creates the principle components or vectors that represent the same data in <strong>lower dimensions</strong>, and vectors are used for training the model further.</p>
<p><img alt="" src="https://pythonkitchen.com/wp-content/uploads/2023/02/pcaimg-300x175.jpg" />
<a href="https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e" title="Image Source">Image Source</a></p>
<p>As we can see in the above image, we have a dataset with three dimensions, X1, X2, and X3. Now here if we want to visualize the data we need to plot the data in 3 axes of the three features, but we can use principle component analysis or PCA which will reduce the dimension of the data by converting its features to only two principle components or principal vectors <strong>PC1 and PC2</strong>. Once the data is reduced to the lower dimensions now, the data can be visualized by plotting it to only two dimensions, PC1 and PC2.</p>
<h1>How Does PCA Works?</h1>
<p>Now that we have a core intuition about the principle component analysis, its use case, and its importance, let us discuss how this technique works and what steps are performed to reduce the dimensionality of the dataset.</p>
<h3>Normalize the Data</h3>
<p>The first step performed before applying the principle component analysis is to normalize the data, basically to bring the data observation values to the <strong>same scale</strong>. Although this step is not necessary, we can also directly apply PCA, but from the experiments, it is proven that applying normalization in PCA helps in many ways.</p>
<h3>Calculate the Covariance Matrix</h3>
<p>The next step is to calculate the covariance matrix for the datasets which will contain the information on the variance of single variables and covariance between double variables in the dataset. To compute this <strong>covariance matrix</strong>, we will take a matrix first, will <strong>transpose</strong> it, and will again multiply it by the actual matrix, which will give us the covariance matrix.</p>
<h3>Calculate Eigen Vectors and Eigenvalues</h3>
<p>The last step is to calculate the <strong>eigenvalues and eigenvectors</strong> of the dataset, where the eigenvectors will help to decide the algorithm the best for principle components or principle vectors to select, as there can be many principle vectors possible in the dataset to reduce the dimensionality of the data.</p>
<h4>Code Example:</h4>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
scale = StandardScaler()
X_train = scale.fit_transform(X_train)
X_test = scale.transform(X_test)

from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

</code></pre>
<h1>Key Takeaways</h1>
<ol>
<li>Curse of dimensionality is a term used when the data haves many dimensions which can increase the computational power of the model and can affect the performance of the model.</li>
<li>Principle component analysis is a technique used for reducing the dimensionality of the data.</li>
<li>Before calculating the vectors that represent the data in lower dimensions, it is advised to normalize the data or make the data values on the same scale.</li>
<li>Principle component analysis uses the principle components or principle vectors derived from the eigenvalues and eigenvectors to reduce the dimensionality of the dataset.</li>
</ol>
<h1>Conclusion</h1>
<p>In this article, we discussed the principle component analysis, the core intuition behind it, and how the principle component analysis works with code examples. This article will help one to understand the concept of principal component analysis better and will help one to apply this technique wherever necessary.</p>

                        <script src="https://utteranc.es/client.js"
        repo="pythonkitchen/blog-comments"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
                    </article>
                
                  </div>
                
            </div>
        </div>
        
    </div>
    <div class="basis-1/4"></div>
  </div>




<script>hljs.highlightAll();</script>




</body>
</html>